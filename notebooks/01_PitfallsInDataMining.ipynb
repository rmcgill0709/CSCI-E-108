{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01 - Pitfalls in Data Mining     \n",
    "## CSCI E-96\n",
    "\n",
    "The goal of data mining is to find important relationships in large complex datasets. These dataset typically contain a large number of variables. The **high-dimensional** nature of the data leads to some commonly encountered pitfalls which lead to incorrect inferences.   \n",
    "\n",
    "In this assignment you will gain a bit of experience with three important concepts in data mining:  \n",
    "\n",
    "1. **False Discovery Rate Control:** The goal of data mining is to find important relationships in large complex datasets. These dataset typically contain a large number of variables. The **high-dimensional** nature of the data leads to some commonly encountered pitfalls of incorrect inference, **false discovery**. A related problem is cutting off a large-scale analysis when a desired relationship is 'found'. This practice of **p-value mining** often leads to unwarranted inferences. You will apply false discovery rate (FDR) control methods to address this problem.   \n",
    "2. **Key-Value Pairs:** Large scale data is typically managed using key-value (KV) pairs. The exercises in this assignment give you some experience working with KV pair data management.  \n",
    "3. **Map and Reduce Processes:** Much of large scale data mining requires use of a split-apply-combine approach. The data is split into manageable chunks, analytic transformations are applied, and the result combined or aggregated. A commonly used class of a split-apply-combine algorithm is MapReduce. \n",
    "\n",
    "In order to keep the scope of this assignment manageable, you will use limited versions of KV pair management and MapReduce. Specifically, you will use common Python tools to implement these concepts rather than dedicated large scale analytic platforms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Hypothesis Tesing\n",
    "\n",
    "Testing multiple hypothesis in high-dimensional data is problematic. Exhaustively testing all pairwise relationships between variables in a data set is a commonly used, but generally misleading, from of **multiple comparisons**. The chance of finding false significance, using such a **data dredging** approach, can be surprisingly high. \n",
    "\n",
    "In this exercise you will perform multiple comparisons on **only 20 identically distributed independent (iid)** variables. Ideally, such tests should not find significant relationships, but the actual result is quite different. \n",
    "\n",
    "To get started, execute the code in the cell below to load the required packages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "from scipy.stats import ttest_ind, f_oneway\n",
    "from itertools import product, combinations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import time\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will apply a [t-test](https://www.statisticshowto.com/probability-and-statistics/t-test/) to all pairwise combinations of identical Normally distributed variables. In this case, we will create a data set with 20 iid Normal distributions of 1000 samples each. Execute the code in the cell below to find this data and display the mean and variance of each variable.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The means of the columns are\n",
      " [-1.16191649e-01  2.80829317e-02 -1.78516419e-02 -1.44691489e-02\n",
      "  3.03718152e-02  1.20007442e-02 -9.58845606e-05  1.98662580e-03\n",
      "  4.94154934e-02 -4.11640866e-02 -6.32977862e-03 -5.93868192e-02\n",
      " -2.56373595e-02  1.43568791e-02 -1.44725765e-02 -1.37023955e-02\n",
      "  1.80622439e-02  5.87029691e-02 -2.02650514e-02 -1.56346106e-02]\n",
      "\n",
      "The variances of the columns are\n",
      " [0.94834508 1.04744241 1.0258018  0.96977571 1.0089001  1.04113864\n",
      " 1.00657222 0.99192594 1.04713487 1.04329434 1.04023108 0.96791346\n",
      " 1.03706907 1.07179865 1.01431404 1.05060289 1.02054329 0.9686211\n",
      " 1.02810287 0.99521555]\n"
     ]
    }
   ],
   "source": [
    "ncolumns = 20\n",
    "nr.seed(234)\n",
    "normal_vars = nr.normal(size=(1000,ncolumns))\n",
    "print('The means of the columns are\\n', np.mean(normal_vars, axis = 0))\n",
    "print('\\nThe variances of the columns are\\n', np.var(normal_vars, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that means and variances are close to 0.0 and 1.0 respectively. As expected, there is not much difference between these variables.\n",
    "\n",
    "How many of these t-tests will show **significance** at the 0.05 cut-off level? With 20 variables there are 190 unique pairwise combinations. We expect to find a number of falsely significant test results from this many pairwise tests. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a hash \n",
    "\n",
    "The goal of this exercise is to compute pairwise hypothesis tests of the differences in means for each of the iid Normal vectors. As an intermediate step you will create a Pandas data frame using a hash of the keys of the vectors. The data frame will contain the **key-value**, $(K,V)$, pairs. Each key must represent an index for the two vectors used to compute the test statistic. The keys will then be used to index the results of these hypothesis tests. \n",
    "\n",
    "The question is, how can we create a hash from the keys for the pair of vectors? In this case to we will use a simple, but far from optimal hash. For the two vector indicies $i, j$, for some key and modulo, $m$, we will compute the hash as:  \n",
    "\n",
    "$$h(i,j) = (i + key*j) mod\\ m$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 1-1:** Given that our space of vectors is actually quite small, just 20, we do not need a sophisticated and scalable hash function. This hashed key will then be used to store and retrieve the values using the indicies of a Pandas data frame, in about $O(N)$ time.     \n",
    "\n",
    "> In this exercise you will test a simple hash function and its inverse. Examine the code below and notice that the hash function encodes the two indexes into a single integer by simple additional and multiplication. The modulo operation limits the size of the hash table. However, to keep things simple you will not need to implement any hash collision resolution mechanism. As a result, the size of the table is set much larger than required.  \n",
    "\n",
    "> To test this hash, do the following:    \n",
    "> 1. Create a function called hash function to compute the hash. The arguments to the function are $i$ and $j$, the $hash\\_key$ and the $modulo\\_multiplier$. The defaults of the arguments are $hash\\_key=128$ and $modulo\\_multiplier=32$. The modulo number is $hash\\_key * modulo\\_multiplier$, e.g. $modulo = 4,096$. The multiplier is the ratio of expected values stored, $n$, to the number of unique hash keys, $m$, e.g. the ratio $m/n$.\n",
    "> 2. Using the Python [ittertools.combinations](https://docs.python.org/3/library/itertools.html#itertools.combinations) function create all unique pairwise combinations of indexes i and j. The arguments to this function are the indexes to the iid Normal vectors. The iterator is `range(ncolumns)` choose 2, since these comparisons are pairwise.    \n",
    "> 3. Within this loop call the hash with the values of $i$ and $j$ as arguments.   \n",
    "> 3. On a single line print the following; the values of i and j, the hash key value, but only if $i \\le 6$. The restriction is to keep the printed output shorter.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def hash_function(i, j, hash_key=128, modulo_multiplier=32):\n",
    "    ## Put your code below. \n",
    "    \n",
    "    return (i + (j + 1) * hash_key) % modulo\n",
    "\n",
    "\n",
    "for i,j in combinations(range(ncolumns), 2):\n",
    "    if i < 3: \n",
    "        hash = hash_function(i,j)\n",
    "        print('i = ' + str(i) + '  j = ' + str(j) + '   hash = ' + str(hash))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the key pairs and the hash values. The question is, are there any hash collisions? This can be done as follows:   \n",
    "> 5. Compute a list of the hash values for all combinations of $i$ and $j$.   \n",
    "> 6. Print the length of the list.  \n",
    "> 7. Print the length of the unique values of the hash. You can find the unique values in a list with the [numpy.unique](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below. \n",
    "\n",
    "print(len(hash_list))\n",
    "print(len(np.unique(hash_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Examine the results you have printed. Is there any evidence of hash key collisions?     \n",
    "> The ratio of $m/n$ is deliberately kept high since the simple hash function has no collision resolution mechanism. Optionally, you can try reducing this ration (the multiplier) to 16 and 8, noting the increase in hash collisions.  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A MapReduce Example   \n",
    "\n",
    "In the following steps you will use a simplified MapReduce framework to find and sort every possible pairwise t-test result for the `normal_vars` table. To do so you must find all **combinations** of keys, ${i,j}. The MapReduce processes you will create, are intended to create a table of these unique pairwise key combinations. The t-test is then computed from the values in the `normal_vars` table in the Reduce process. This MapReduce process only creates the required key combinations. The values remain in the `normal_vars` table. This approach simplifies some of the bookkeeping.     \n",
    "\n",
    "In a bit more details the steps for this algorithm are:   \n",
    "- The **Map process** builds a hash table with the hash of i,j as key and i,j as values. As has been noted most of this hash table will contain NaNs. Please don't be confused by the switching roles of i,j as they act as keys and values at different times.   \n",
    "- The **Shuffle** scans the hash table for non-empty rows (not NaN values for i,j) and builds a table of unique combination of keys i and j.   \n",
    "- The **Reduce process** uses the keys i,j from the Shuffle process to look up the columns of the numeric random variables in the `normal_vars` table. The resulting table contains the i,j pairs as the keys for the random variable lookup. The t-test is applied to these numeric values. The results are sorted and returned.   \n",
    "\n",
    "It is clear that at this small scale one could compute the t-test results for the combinations of the random variables directly. The point of the exercises is to get some experience working with key-value pairs and the MapReduce framework. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The map process\n",
    "\n",
    "We are constructing this example a map and a reduce process. The processes are intended to compute the hypothesis test for differences of means between all the pairs of vectors. The first step is the map process, which creates the keys, or values of $i$ and $j$ for these pairs. This is an example of a **similarity join**.    \n",
    "\n",
    "> **Computational Note:** In this exercise you will create your own simple hash table using a Pandas data frame. The data frame will be indexed by the key value computed by hashing the $[i,j]$ pairs, which gives nearly $O(n)$ performance for lookups. This implementation is only for demonstration purposes only and has several significant limitations compared to a production quality implementation. \n",
    "> 1. You will not need to implement a collision resolution mechanism so a larger table is required, which is not particularly memory efficient.    \n",
    "> 2. You will not implement any table resizing mechanism. Again, a fixed size table is limited in the quantity of data the table can store and also resulting in poor memory utilization.  \n",
    "> As an alternative, we could use the built hashed $[K,V]$ pairs of a Python dictionary. However, a dictionary only allows one key value and we are working with 2, $[i,j$. You could create a hash and then hash this hash with the built in function. In this care we will take the more direct route just described. If you are not familiar with Python dictionaries you can find a short tutorial [here](https://www.tutorialspoint.com/python_data_structure/python_hash_table.htm), as well as many other places on the web.   \n",
    "\n",
    "> **Exercise 1-2:** You will now create the code for the map which build a data frame with $i, j$ key pairs indexed by the hash. By the following steps you will create code that represents a map task.  a\n",
    "> 1. Create a data frame with two columns $i$ and $j$ with number of rows $= hash_key * modulo_multiplier $ and set all values to $= numpy.nan$.\n",
    "> 2. Create a loop over all combinations of the pairs of i and j.   \n",
    "> 2. Compute the hash key value for the indexes, i and j.  \n",
    "> 4. Add the $i$ and $j$ values to the row indeed by the hash key.  \n",
    "> 5. Return the hash table.\n",
    "> 6. Execute the function to create the hash table.\n",
    "> 7. Compute and print the length of the hash table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_hypothesis(vars, hash_key=128, modulo_multiplier=32):\n",
    "    ## Put your code below. \n",
    "    \n",
    "    \n",
    "    \n",
    "    for i,j in combinations(range(ncolumns), 2): \n",
    "        ## Put your code below. \n",
    "        ## Compute the hash key and added the values to the \n",
    "        ## row of the data frame. \n",
    "        \n",
    "        \n",
    "    return hash_table\n",
    "\n",
    "hash_table = map_hypothesis(normal_vars)\n",
    "hash_table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The shuffle and reduce task\n",
    "\n",
    "Now that you have the keys for the pairwise combinations of the vectors it is time to perform the reduce process. The reduce process computes the pair-wise t-statistics and p-values. These statistical values are indexed by the keys of the pair of vectors. This process reduces the full vectors of values down to just two numbers for each pair of vectors. \n",
    "\n",
    "> **Exercise 1-3:** You will now create and apply the following code for the reduce process:   \n",
    "> 1. Create an empty data frame with columns, `i`, `j`, `t_statistic`, and `p_value`.    \n",
    "> 2. Using a for loop iterate over all possible (hashed) keys of the data frame. An if statement is used to test if these are valid values of the key, i. Use the [numpy.isnan](https://numpy.org/doc/stable/reference/generated/numpy.isnan.html) function for this test.  \n",
    "> 3. Extract the values of i and j from the input data frame. \n",
    "> 4. Using the keys, look up the values and use the values to compute the t-statistic and p-value with the [scipy.stats import ttest_ind](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) function.\n",
    "> 5. Assign the values to the row in the data frame. Use the `hash_function` previously created to find the (hashed) row index of the data frame for the assignment, which will work with the Pandas `loc` method.     \n",
    "> 6. Return the data frame, sorted in ascending order, using the [Pandas.DataFrame.sort_values](https://turned.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html).    \n",
    "> 7. Execute your function and save the returned data frame.   \n",
    "> 8. Display the head of the data frame.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def reduce_significance(hash_table, values):  \n",
    "    ## Create a data framreturn the results of the \n",
    "    ## the reduce process. The results are grouped by the first \n",
    "    ## index i. \n",
    "    test_results = pd.DataFrame(index=hash_table.index, columns=['i','j','t_statistic','p_value'])\n",
    "\n",
    "    ## As a substitute for a shuffle we will use a simple search \n",
    "    ## through the data frame  \n",
    "    for hash in range(hash_table.shape[0]): \n",
    "    ## Put your code below. \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    ## Given the i,j pairs we need to compute the t-statistic and the p-value.   \n",
    "    ## This is the reduce step, since for each i,j pair there is only \n",
    "    ## a t-statistic and a p-value. \n",
    "    ## Put your code below. \n",
    "            \n",
    "        \n",
    "    ## Sort and return the results\n",
    "    #test_results.sort_values('p_value', axis=0, ascending=True# , inplace=True)\n",
    "    return test_results.sort_values('p_value', axis=0, ascending=True)      \n",
    "        \n",
    "test_stats = reduce_significance(hash_table, normal_vars)       \n",
    "test_stats.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 9. In the cell below, create a filter for pair test cases which are significant and save these cases in a data frame. \n",
    "> 10. Print the number (len) of significant results.\n",
    "> 11. Print the rows with the significant test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "significance_level = 0.05\n",
    "## Put your code below. \n",
    "\n",
    "\n",
    "print(len(significant_tests))\n",
    "print(significant_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Notice the large number of apparently significant tests. Answer the following questions:  \n",
    "> 1. Is the number of false positive cases approximately what you expected?    \n",
    "> 2. Examine which of the iid Normal vectors contribute to the false positive results. Are there vectors which contribute multiple times?   \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Answers:**   \n",
    "> 1.       \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonferroni correction  \n",
    "\n",
    "Several adjustments to the multiple comparisons problem have been proposed. In Dunn published a method know as the **Bonferroni correction** in 1961. The Bonferroni correction is a widely used method to reduce the false positive rate of hypothesis tests.  The adjustment is simple:\n",
    "$$\\alpha_b = \\frac{\\alpha}{m}\\\\\n",
    "with\\\\ \n",
    "m =\\ number\\ of\\ groups$$\n",
    "\n",
    "Can the Bonferroni correction help? Yes, by greatly increasing the confidence level required for a statistically significant result. The problem with the Bonferroni correction is the reduction in power as the  grows smaller. For big data problems with large numbers of groups, this issue can be especially serious. \n",
    "\n",
    "> **Exercise 1-4:** You will now apply the Bonferroni correction to determining the significance of the p-values of the pair-wise t-tests. Do the following:\n",
    "> 1. Compute the Bonferroni significance level. \n",
    "> 2. Apply this cutoff to the p-values of all of the cases.\n",
    "> Execute your code and examine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "len(significant_tests)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Does using the  Bonferroni correction reduce false significance tests, or eliminate even eliminate them?           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> But, can we detect small effect with Bonferroni correction, as this method significantly reduces power of tests? Execute the code in the cell below, which compares a standard Normal to a Normal with a small mean (effect size), to find out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr.seed(567)\n",
    "print(significance_bonferroni)\n",
    "ttest_ind(normal_vars[:,0], nr.normal(loc = 0.01, size=(1000,1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Provide a short answer to these questions:        \n",
    "> 1. Given the Bonferroni correction, this difference in means would this result be considered significant or would it result in a type II error, and why?    \n",
    "> 2. What does this result tell you about the downside of the Bonferroni correction as a FDR control method, in terms of finding significant results?      \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answers:**      \n",
    "> 1.      \n",
    "> 2.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False Discovery Rate Control Methods \n",
    "\n",
    "We have seen the potential pitfalls of multiple hypothesis testing. Further, we have seen that a simple approach to **false discovery rate (FDR) control** is not effective. You will now apply more sophisticated FDR control methods. \n",
    "\n",
    "The specific problem we will explore is to determine which genes lead to expression of a certain disease. In this example, there are gene expression data for 97 patients. Some of these patients have ulcerative colitis, a condition believed not to be inheritable, and others have Crohn's disease, which is believed to be genetically inherited.    \n",
    "\n",
    "One approach to this problem is to perform hypothesis tests on the expression of the genes between patients with the two conditions. Since there are over 10,000 genes there is considerable chance for false discovery. Therefore, careful application of FDR control is required.\n",
    "\n",
    "To continue with the example, execute the code in the cell below to load the data and print the dimensionality of the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_data = pd.read_csv('../data/ColonDiseaseGeneData-Cleaned.csv')\n",
    "print(gene_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are data from 97 patients (rows) with 10,497 genes (features in columns). A large number of pairwise hypothesis tests are required!     \n",
    "\n",
    "Execute the code in the cell below to view the first 5 columns of the data frame, which includes the expression of the first 4 genes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print(gene_data.iloc[:,:5])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holm's method\n",
    "\n",
    "You will apply two FDR control methods to these data to reduce the FDR rate, while not being overly conservative on true positive results, like the Bonferronic correction. The first of these **Holm's method**.    \n",
    "\n",
    "The Holm's method operates on the ordered set of p-values, $D = \\{ p_{(1)}, p_{(2)}, p_{(3)}, \\ldots, p_{(n)} \\}$. The threshold for the $ith$ p-value, $p(i)$ is:  \n",
    "\n",
    "$$p(i) \\le Threshold(Holm's) = \\frac{\\alpha}{N - i + 1}$$\n",
    "\n",
    "For example: for the 10th ordered p-value with 1,000 total tests (genes) and significance level of 0.05, the cutoff is:   \n",
    "\n",
    "$$p(10) \\le \\frac{0.05}{1000 - 10 + 1} = 0.00005045$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map process  \n",
    "\n",
    "> **Exercise 01-4:** To start the processing of these data you will first create and execute code for a map process. The map process groups the data by the patient's disease into data frame, ulcerative, crohns. The keys for each of these key-value pairs are the gene identifier. Notice that one key is all that is needed in this case. Now do the following to create and execute a function, `map_gene`:   \n",
    "> 1. Create a logical mask and group the values by `Disease State` into two data frames.\n",
    "> 2. Return the transpose of the two data frames, removing the `Disease State` values. The result of this operation should be data frames with gene expressions in the columns and the gene identifier as the row index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_gene(gene_data):  \n",
    "    ## First, separate the columns by disease type  \n",
    "    ## Put your code below. \n",
    "    \n",
    "    \n",
    "    \n",
    "    return ulcerative.iloc[:,1:].transpose(), crohns.iloc[:,1:].transpose()\n",
    "    \n",
    "ulcerative, crohns = map_gene(gene_data)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 3. Execute the code in the two cells below to display the heads of these data frames and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulcerative.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crohns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce process \n",
    "\n",
    "> **Exercise 01-5:** With the key-value pairs organized by disease state, it is time to create and execute code of a reduce process. The reduce process will compute the pairwise t-statistics and p-values for each gene and return the sorted results. Specifically, your `gene_test` with arguments of the two mapped data frames will do the following:   \n",
    "> 1. Create an empty results data frame with using the hash key as an index and columns t_statistics, and p-value.\n",
    "> 2. Use a `for` loop to iterate over the keys of either of the data frames.  \n",
    "> 3. Compute the t-statistic and p-value for the gene (key) for the pairwise tests between the data frames.\n",
    "> 4. Assign the values to the row in the data frame. Use the `hash_function` previously created to find the (hashed) row index of the data frame for the assignment, which will work with the Pandas `loc` method.    \n",
    "> 5. Sort the results data frame, `inplace`, into ascending order.\n",
    "> 6. Return the resulting data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gene_test(ulcerative, crohns):  \n",
    "    test_results = pd.DataFrame(columns=['gene','t_statistic','p_value'])\n",
    "    ## Put your code below. \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return test_results.set_index('gene')\n",
    "    \n",
    "gene_statistics = gene_test(ulcerative, crohns)    \n",
    "gene_statistics.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance of results \n",
    "\n",
    "With the gene data reduced to the t-test statistics, you will now determine the significance of these tests. It is important to understand that scientists believe that expression of a disease, like Corhn's, is only in a small number of genes, less than approximately 100.  \n",
    "\n",
    "> **Exercise 01-6:** As a first step in understanding the gene expression significance complete and execute the code in the cell below to find the number of 'significant' genes using the simple single hypothesis test cutoff criteria.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_level =0.05\n",
    "## Put your code below. \n",
    "\n",
    "len(significant_tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Does this large number of 'statistically significant' results appear credible, given that only a few genes are thought to have significant expression for this disease and why in terms of FDR?    \n",
    "> **End of exercise.**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 01-7:** We have already seen that the Bonferroni correction is a rather conservative approach to testing the significance of large numbers of hypotheses. You will now use the Bonferroni correction to test the significance of the gene expression, by completing the code in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Put your code below. \n",
    "\n",
    "\n",
    "len(significant_Bonferroni)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Does the foregoing result seems reasonable in terms of discovery rate and FDR control?    \n",
    "> **End of exercise.**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 01-08:** It may well be the case that the foregoing results using the Bonferroni correction is too conservative. You will now apply the Holms method to determining significance of the gene expression test results. In the cell below complete the `holms_significance` function with arguments of the results data frame and the significance level. This function does the following:  \n",
    "> 1. Find the number of test results and compute the numerator used for the cutoff calculation. \n",
    "> 2. Compute the vector of thresholds using the Holms formula. Use the Python `range`function to get the values of the index i. But, keep in mind that range produces a zero-indexed iterator, and the algorithm needs a one-indexed list.  Use the [numpy.divide](https://numpy.org/doc/stable/reference/generated/numpy.divide.html) function to perform the vector divide. Save these threshold values in a data frame in a 'holms_threshold' column.   \n",
    "> 3. Using the threshold values compute a logical vector and save it in a logical column named 'significance' in the data frame.\n",
    "> 4. Return the data frame.\n",
    "> Finally, execute the function and save the results in a data frame. Then find the length of the subset where the 'significance' value is True. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holms_significance(test_results, significance):\n",
    "    ## First compute the thresholds for each of the ordered tests\n",
    "    ## Put your code below. \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    return test_results\n",
    "\n",
    "holms_results = holms_significance(gene_statistics, significance_level)    \n",
    "len(holms_results.loc[holms_results.loc[:,'significant'],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Consider the differences between the calculation of the Bonferroni correction and Holm's threshold. Do you expect any practical difference for large number of test and small test index, $i$? Does this relationship explain the similarity of the results in this case?  \n",
    "> **End of exercise.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the results of the Holm's method test. The plot has two key elements:  \n",
    "1. Plot the curve of the p-values vs. the order number, i. The line is color coded by significance or not.\n",
    "2. Plot the threshold line. This line is straight since the threshold is a linear function of i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_significance(results, threshold):\n",
    "    results['number'] = range(len(results))\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.lineplot(x='number',y=threshold, data=results, ax=ax, color='black', linewidth=0.5)\n",
    "    sns.scatterplot(x='number',y='p_value', hue='significant', data=results, s=3, ax=ax)\n",
    "    ax.set_title('Significance of gene expression')\n",
    "    ax.set_xlabel('Gene number')\n",
    "    ax.set_ylabel('p-value')\n",
    "    \n",
    "plot_significance(holms_results.iloc[:150,:].copy(), 'holms_threshold')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the following about this plot:  \n",
    "1. The p-value significance line crosses the threshold point at an apparent break point.   \n",
    "2. The significant p-values are all very small since there are so many tests.\n",
    "3. The Holm's threshold is does not change very much over the first 150 test results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benamini-Hochberg FDR Control \n",
    "\n",
    "The Benamini-Hochberg FDR control algorithm is another way to control false discoveries. Stat with an ordered set of $n$ p-values, $D = \\{ p_{(1)}, p_{(2)}, p_{(3)}, \\ldots, p_{(n)} \\}$ we define a false discovery rate, $q$:\n",
    "\n",
    "$$FDR(D) \\le q$$\n",
    "\n",
    "The cutoff threshold for the ith p-value is then:\n",
    "$$p_{(i)} \\le Threshold(D_q) = \\frac{q}{n} i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 01-9:** In this exercise you will apply the Benamini-Hochberg FDR control algorithm for testing the significance of the gene expressions. The `BH_significance` function is quite similar to the Holm's method function you have already created. Given the large number of genes you must use a low false discovery rate, $0.001$, or 1 out of 1,000. \n",
    "> Execute your function, saving the result. Then print the number of significant cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BH_significance(test_results, false_discovery_tollerance):\n",
    "    ## First compute the thresholds for each of the ordered tests\n",
    "    ## Put your code below. \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    return test_results\n",
    "\n",
    "BH_results = BH_significance(gene_statistics, 0.001)    \n",
    "len(BH_results.loc[BH_results.loc[:,'significant'],:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This result differs from the first two FDR control methods you have applied. Given the false discovery parameter of 0.001 do you think this is a reasonable result and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, execute the code in the cell below and examine the resulting plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_significance(BH_results.iloc[:150,:].copy(), 'bh_threshold')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Exercise 01-10**: Compare the plots of results of the Benamini-Hochberg FDR control method to those of Holm's method. Why does the Benamini-Hochberg FDR control method give a more dynamic or adaptive result for the case where $i << n$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Answer:**   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Copyright 2021, 2022, 2023, Stephen F. Elston. All rights reserved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f8fcbc9378323652c21ee42b0d3ec8e3ceff44abf567b8ac4d29b87c8f10302a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
